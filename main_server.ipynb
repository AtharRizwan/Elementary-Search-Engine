{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848a23a9-e2a5-43be-9351-5b2741f2f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings \n",
    "import json\n",
    "import flask_cors, flask\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from flask_cors import CORS\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "from io import BytesIO\n",
    "import re\n",
    "import base64\n",
    "from PIL import Image \n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import validators\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('stopwords')\n",
    "# Initialize Word_Net_Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733b7fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  6.83it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.24.0\",\n",
       "  \"_name_or_path\": \"stabilityai/sd-turbo\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c5158f-2753-4320-a38b-396d01326079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8558123",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_barrels = 1000\n",
    "# Load lexicon\n",
    "lexicon = load_data_from_json(r\"Files\\\\lexicon.json\")\n",
    "# Load the documents\n",
    "documents = load_data_from_json(r\"Files\\\\documents.json\")\n",
    "barrel_files = os.listdir(r\"Files\\Barrels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3ab3ce-456d-483d-a01a-65594f1fccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in content, preprocesses it, \n",
    "# and converts it to a list of words\n",
    "def pre_process_string(content):\n",
    "    # Remove \\n and \\t\n",
    "    content = content.replace('\\n', ' ')\n",
    "    content = content.replace('\\t', ' ')\n",
    "    # Remove all non-characters\n",
    "    content = re.sub(r'[^a-zA-Z\\s]', ' ', content)\n",
    "    # Remove multiple spaces\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    # Convert all characters to lowercase\n",
    "    content = content.lower()\n",
    "    # Convert the title into a list of words\n",
    "    content = content.split()\n",
    "    # Remove one and two character words\n",
    "    content = [word for word in content if len(word) > 2]\n",
    "    # Remove stop_words using nltk\n",
    "    content = [word for word in content if not word in stopwords.words('english')]\n",
    "    return content\n",
    "    \n",
    "# Function that takes in a list of words and adds them to the lexicon\n",
    "def build_lexicon(words):\n",
    "    # Build the lexicon\n",
    "    new_words = []\n",
    "    # Look through the words\n",
    "    for word in words:\n",
    "        # Lemmatize the word\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        # if that word is not already in lexicon\n",
    "        if word not in lexicon and word not in new_words:\n",
    "            # Then add it\n",
    "            new_words.append(word)\n",
    "    lexicon.extend(new_words)\n",
    "    return\n",
    "\n",
    "# Function to build forward index from raw articles\n",
    "def build_forward_index(articles):\n",
    "\n",
    "    # initialize forward_index\n",
    "    forward_index = dict()\n",
    "\n",
    "    #initialize documents\n",
    "    docs = dict()\n",
    "\n",
    "    # Load the already existing forward_index\n",
    "    try:\n",
    "        data = load_data_from_json(r\"Files\\\\forward_index.json\")\n",
    "    except:\n",
    "        with open(r\"Files\\\\forward_index.json\", \"w\") as file:\n",
    "            json.dump(dict(), file)\n",
    "        data = load_data_from_json(r\"Files\\\\forward_index.json\")\n",
    "        \n",
    "    num_articles = len(documents)\n",
    "    \n",
    "    # Extract all urls currently indexed\n",
    "    try:\n",
    "        article_urls = [article['url'] for article in documents.values()]\n",
    "    except:\n",
    "        article_urls = []\n",
    "        \n",
    "    # For each article\n",
    "    for article in articles:\n",
    "        # if article is not already forward indexed\n",
    "        if article['url'] not in article_urls:\n",
    "            # Pre-process the title and content\n",
    "            title_words = pre_process_string(article['title'])\n",
    "            content_words = pre_process_string(article['content'])\n",
    "            # Update the lexicon\n",
    "            build_lexicon(title_words + content_words)\n",
    "            # Lemmatize the words in content and title\n",
    "            content_words = [lemmatizer.lemmatize(word) for word in content_words]\n",
    "            title_words = [lemmatizer.lemmatize(word) for word in title_words]\n",
    "            # Convert the words in title and content to their respective indexes\n",
    "            content_ids = [lexicon.index(word) for word in content_words]\n",
    "            title_ids = [lexicon.index(word) for word in title_words]\n",
    "            # Count the frequencies of words\n",
    "            frequency = Counter((title_ids * 10) + content_ids)\n",
    "            forward_index[num_articles] = frequency\n",
    "            docs[str(num_articles)] = {'title': article['title'], 'url': article['url']}\n",
    "            # Add the url to the article\n",
    "            article_urls.append(article['url'])\n",
    "            num_articles += 1\n",
    "            \n",
    "    data.update(forward_index)\n",
    "    print(docs)\n",
    "    documents.update(docs)\n",
    "    # Update the lexicon json file\n",
    "    with open(r\"Files\\lexicon.json\", \"w\") as file:\n",
    "        json.dump(lexicon, file)\n",
    "    # Update the forward_index json file\n",
    "    with open(r\"Files\\forward_index.json\", \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "    # Update the documents json file\n",
    "    with open(r\"Files\\documents.json\", \"w\") as file:\n",
    "        json.dump(documents, file)\n",
    "\n",
    "def build_inverted_index_with_barrels():\n",
    "\n",
    "    # Load the forward index\n",
    "    try:\n",
    "        forward_index = load_data_from_json(r\"Files\\forward_index.json\")\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    barrels = [None] * num_barrels\n",
    "\n",
    "    # Iterate through all articles in the forward_index\n",
    "    for doc_id, data in forward_index.items():\n",
    "        # Look at all words in an article\n",
    "        for word_id in data:\n",
    "            # Calculate the barrel number for that word\n",
    "            barrel_no = int(word_id) % num_barrels\n",
    "            barrel_filename = f\"barrel_{str(barrel_no).zfill(5)}.json\"\n",
    "            \n",
    "            # Check if that barrel exists, if not then create it\n",
    "            barrel_path = os.path.join(r\"Files\\Barrels\", barrel_filename)\n",
    "            if not os.path.exists(barrel_path):\n",
    "                with open(barrel_path, \"w\") as file:\n",
    "                    json.dump(dict(), file)\n",
    "                barrel_files.append(barrel_filename)\n",
    "            # Load the barrel\n",
    "            if barrels[barrel_no] is None:\n",
    "                barrels[barrel_no] = (load_data_from_json(barrel_path))\n",
    "                \n",
    "            # If that word is not already in that barrel\n",
    "            if word_id not in barrels[barrel_no]:\n",
    "                # Then create a dict at that word_id\n",
    "                barrels[barrel_no][word_id] = dict()\n",
    "            # And add the doc_id for that word along with frequency if it is not already there\n",
    "            if doc_id not in barrels[barrel_no][word_id]:\n",
    "                barrels[barrel_no][word_id].update({doc_id: data[word_id]})\n",
    "\n",
    "    # Update the loaded barrels\n",
    "    for i, barrel in enumerate(barrels):\n",
    "        if barrel is not None:\n",
    "            with open(os.path.join(r\"Files\\Barrels\", barrel_files[i]), \"w\") as file:\n",
    "                json.dump(barrel, file)\n",
    "    \n",
    "    # # Clear the forward_index\n",
    "    with open(r\"Files\\forward_index.json\", \"w\") as file:\n",
    "        json.dump(dict(), file)\n",
    "\n",
    "\n",
    "def rank_results(search_result): \n",
    "     # Rank these documents\n",
    "    # Sort the dictionary by values (descending order)\n",
    "    sorted_tuples = sorted(search_result.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convert the sorted list of tuples back to a dictionary\n",
    "    ranked_result = dict(sorted_tuples)\n",
    "    # Extract the article ids\n",
    "    ranked_articles = ranked_result.keys()\n",
    "    ranked_articles = list(ranked_articles)\n",
    "    \n",
    "    return ranked_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed9c87b-9610-4963-8b46-47d35b661043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/search_1\", methods=[\"GET\"], endpoint='single_word_search')\n",
    "def single_word_search():\n",
    "    word = request.args.get('word')\n",
    "\n",
    "    # Lemmatize the word\n",
    "    word = word.lower()\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    # Find the id of the word in lexicon\n",
    "    try:\n",
    "        word_id = lexicon.index(word)\n",
    "        # Calculate the barrel of the word\n",
    "        barrel_no = word_id % num_barrels\n",
    "        # Load the corresponding barrel\n",
    "        barrel_filename = f\"barrel_{str(barrel_no).zfill(5)}.json\"\n",
    "        barrel_path = os.path.join(r\"Files\\Barrels\", barrel_filename)\n",
    "        barrel = load_data_from_json(barrel_path)\n",
    "        # Find out in which documents does the word appear\n",
    "        search_result = barrel[str(word_id)]\n",
    "    except:\n",
    "        search_result = None\n",
    "    \n",
    "    if search_result is None: \n",
    "        return []\n",
    "\n",
    "    article_ids = rank_results(search_result)\n",
    "    titles = [documents[article]['title'] for article in article_ids]\n",
    "    urls = [documents[article]['url'] for article in article_ids]\n",
    "    \n",
    "    json_response = jsonify(article_ids=article_ids, titles=titles, urls=urls)\n",
    "\n",
    "    return json_response\n",
    "\n",
    "@app.route(\"/search_2\", methods=[\"GET\"], endpoint='multi_word_search')\n",
    "def multi_word_search(): \n",
    "    query = request.args.get('word')\n",
    "        # Preprocess the query\n",
    "    words = pre_process_string(query)\n",
    "\n",
    "    # Remove those words that are not in lexicon\n",
    "    words = [word for word in words if word in lexicon]\n",
    "    # Convert each word to its word_id\n",
    "    word_ids = [lexicon.index(word) for word in words]\n",
    "    # Calculate barrel_no of each word\n",
    "    barrel_nos = [word_id % num_barrels for word_id in word_ids]\n",
    "\n",
    "    # Load the necessary barrels\n",
    "    barrels = [None] * num_barrels\n",
    "    for barrel_no in barrel_nos:\n",
    "        # If a barrel isn't already loaded, then load it\n",
    "        if barrels[barrel_no] == None:\n",
    "            barrel_filename = f\"barrel_{str(barrel_no).zfill(5)}.json\"\n",
    "            barrel_path = os.path.join(r\"Files\\Barrels\", barrel_filename)\n",
    "            barrels[barrel_no] = load_data_from_json(barrel_path)\n",
    "            \n",
    "    # Check the first word\n",
    "    if word_ids:\n",
    "        result = barrels[barrel_nos[0]][str(word_ids[0])]\n",
    "        # Check the rest of the words\n",
    "        for i, word_id in enumerate(word_ids[1:], start = 1):\n",
    "            # Produce the result for current word\n",
    "            current_result = barrels[barrel_nos[i]][str(word_id)]\n",
    "            # Include those articles that are also in the result of current word\n",
    "            result.update({d:result[d]+current_result[d] for d in result.keys() if d in current_result.keys()})\n",
    "\n",
    "    if result is None:\n",
    "        return []\n",
    "    \n",
    "    # rank the results\n",
    "    result = rank_results(result)\n",
    "\n",
    "    article_ids = result\n",
    "    titles = [documents[article]['title'] for article in article_ids]\n",
    "    urls = [documents[article]['url'] for article in article_ids]\n",
    "\n",
    "    json_response = jsonify(article_ids=article_ids, titles=titles, urls=urls)\n",
    "\n",
    "    return json_response\n",
    "\n",
    "\n",
    "@app.route(\"/gen\", methods=[\"GET\"], endpoint='genai_tool')\n",
    "def genai_tool():\n",
    "    word = request.args.get('word') \n",
    "    image = pipe(prompt=word, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "\n",
    "    image2 = image.convert(\"RGB\")\n",
    " \n",
    "    image_bytes_io = BytesIO()\n",
    "    image2.save(image_bytes_io, format=\"PNG\")\n",
    "    image_bytes = image_bytes_io.getvalue()\n",
    "    \n",
    "    image_base64 = base64.b64encode(image_bytes).decode('utf-8')  \n",
    "\n",
    "    json_response = {\n",
    "        'word': word,\n",
    "        'image': image_base64\n",
    "    }\n",
    "\n",
    "    return jsonify(json_response)\n",
    "\n",
    "UPLOAD_FOLDER = 'Files\\\\Uploads'  \n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "@app.route(\"/add\", methods=[\"POST\"], endpoint='add_content')\n",
    "def add_content():\n",
    "    title = request.form.get('title')\n",
    "    url = request.form.get('url')\n",
    "    content = request.form.get('content')\n",
    "    file = request.files.get('file') \n",
    " \n",
    "    # Check if a file is uploaded\n",
    "    if file:\n",
    "        filename = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)\n",
    "        file.save(filename)\n",
    "        try:\n",
    "            # Load the file\n",
    "            data = load_data_from_json(filename) \n",
    "        except:\n",
    "            return jsonify({\"message\": \"Error loading file\"}), 500\n",
    "    else:\n",
    "        # Check if the url, title and content are correct\n",
    "        if url and title and content:\n",
    "            # Validate the url\n",
    "            if not validators.url(url):\n",
    "                return jsonify({\"message\": \"Please provide a valid url\"}), 400\n",
    "            # Load the data\n",
    "            data = [{\"title\":title, \"content\":content, \"url\":url}]\n",
    "        else:\n",
    "            return jsonify({\"message\": \"Please provide a file or url, title and content\"}), 400\n",
    "\n",
    "    # build forward and inverted index on it\n",
    "    try:\n",
    "        build_forward_index(data)\n",
    "    except:\n",
    "        return jsonify({\"message\": \"Error building forward index\"}), 500\n",
    "    \n",
    "    build_inverted_index_with_barrels()\n",
    "    \n",
    "    return jsonify({\"message\": \"Successfully added content\"}), 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76baeb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
