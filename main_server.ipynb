{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848a23a9-e2a5-43be-9351-5b2741f2f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings \n",
    "import json\n",
    "import flask_cors, flask\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from flask_cors import CORS\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "from io import BytesIO\n",
    "import re\n",
    "import base64\n",
    "from PIL import Image \n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import validators\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('stopwords')\n",
    "# Initialize Word_Net_Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7097cb24-60e8-4346-9b4c-2fc99b512cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  3.97it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.24.0\",\n",
       "  \"_name_or_path\": \"stabilityai/sd-turbo\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c5158f-2753-4320-a38b-396d01326079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ab3ce-456d-483d-a01a-65594f1fccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Function that takes in content, preprocesses it, \n",
    "# and converts it to a list of words\n",
    "def pre_process_string(content):\n",
    "    # Remove \\n and \\t\n",
    "    content = content.replace('\\n', ' ')\n",
    "    content = content.replace('\\t', ' ')\n",
    "    # Remove all non-characters\n",
    "    content = re.sub(r'[^a-zA-Z\\s]', ' ', content)\n",
    "    # Remove multiple spaces\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    # Convert all characters to lowercase\n",
    "    content = content.lower()\n",
    "    # Convert the title into a list of words\n",
    "    content = content.split()\n",
    "    # Remove one and two character words\n",
    "    content = [word for word in content if len(word) > 2]\n",
    "    # Remove stop_words using nltk\n",
    "    content = [word for word in content if not word in stopwords.words('english')]\n",
    "    return content\n",
    "    \n",
    "# Function that takes in a list of words and adds them to the lexicon\n",
    "def build_lexicon(words, lexicon):\n",
    "    # Build the lexicon\n",
    "    new_words = []\n",
    "    # Look through the words\n",
    "    for word in words:\n",
    "        # Lemmatize the word\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        # if that word is not already in lexicon\n",
    "        if word not in lexicon and word not in new_words:\n",
    "            # Then add it\n",
    "            new_words.append(word)\n",
    "    lexicon.extend(new_words)\n",
    "    return lexicon\n",
    "\n",
    "# Function to build forward index from raw articles\n",
    "def build_forward_index(articles):\n",
    "    # initialize forward_index\n",
    "    forward_index = dict()\n",
    "\n",
    "    #initialize documents\n",
    "    docs = dict()\n",
    "\n",
    "    # Load the already existing forward_index\n",
    "    try:\n",
    "        data = load_data_from_json(r\"Files\\forward_index.json\")\n",
    "    except:\n",
    "        with open(r\"Files\\forward_index.json\", \"w\") as file:\n",
    "            json.dump(dict(), file)\n",
    "        data = load_data_from_json(r\"Files\\forward_index.json\")\n",
    "        \n",
    "    # Load the lexicon\n",
    "    try:\n",
    "        lexicon = load_data_from_json(r\"Files\\lexicon.json\")\n",
    "    except:\n",
    "        with open(r\"Files\\lexicon.json\", \"w\") as file:\n",
    "            json.dump(list(), file)\n",
    "        lexicon = load_data_from_json(r\"Files\\lexicon.json\")\n",
    "\n",
    "    # Load the documents\n",
    "    try:\n",
    "        documents = load_data_from_json(r\"Files\\documents.json\")\n",
    "    except:\n",
    "        with open(r\"Files\\documents.json\", \"w\") as file:\n",
    "            json.dump(dict(), file)\n",
    "        documents = load_data_from_json(r\"Files\\documents.json\")\n",
    "        \n",
    "    num_articles = len(documents)\n",
    "    \n",
    "    # Extract all urls currently indexed\n",
    "    try:\n",
    "        article_urls = [article['url'] for article in documents.values()]\n",
    "    except:\n",
    "        article_urls = []\n",
    "        \n",
    "    # For each article\n",
    "    for article in articles:\n",
    "        # if article is not already forward indexed\n",
    "        if article['url'] not in article_urls:\n",
    "            # Pre-process the title and content\n",
    "            title_words = pre_process_string(article['title'])\n",
    "            content_words = pre_process_string(article['content'])\n",
    "            # Update the lexicon\n",
    "            lexicon = build_lexicon(title_words + content_words, lexicon)\n",
    "            # Lemmatize the words in content and title\n",
    "            content_words = [lemmatizer.lemmatize(word) for word in content_words]\n",
    "            title_words = [lemmatizer.lemmatize(word) for word in title_words]\n",
    "            # Convert the words in title and content to their respective indexes\n",
    "            content_ids = [lexicon.index(word) for word in content_words]\n",
    "            title_ids = [lexicon.index(word) for word in title_words]\n",
    "            # Count the frequencies of words\n",
    "            frequency = Counter((title_ids * 10) + content_ids)\n",
    "            forward_index[num_articles] = frequency\n",
    "            docs[num_articles] = {'title': article['title'], 'url': article['url']}\n",
    "            # Add the url to the article\n",
    "            article_urls.append(article['url'])\n",
    "            num_articles += 1\n",
    "    data.update(forward_index)\n",
    "    documents.update(docs)\n",
    "    # Update the lexicon json file\n",
    "    with open(r\"Files\\lexicon.json\", \"w\") as file:\n",
    "        json.dump(lexicon, file)\n",
    "    # Update the forward_index json file\n",
    "    with open(r\"Files\\forward_index.json\", \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "    # Update the documents json file\n",
    "    with open(r\"Files\\documents.json\", \"w\") as file:\n",
    "        json.dump(documents, file)\n",
    "\n",
    "def build_inverted_index_with_barrels():\n",
    "    # Load the forward index\n",
    "    try:\n",
    "        forward_index = load_data_from_json(r\"Files\\forward_index.json\")\n",
    "    except:\n",
    "        with open(r\"Files\\forward_index.json\", \"w\") as file:\n",
    "            json.dump(dict(), file)\n",
    "        forward_index = load_data_from_json(r\"Files\\forward_index.json\")\n",
    "\n",
    "    barrels = []\n",
    "    barrel_files = os.listdir(r\"Files\\Barrels\")\n",
    "    # Load all barrels that currently exist\n",
    "    for barrel in barrel_files:\n",
    "        barrels.append(load_data_from_json(os.path.join(r\"Files\\Barrels\", barrel)))\n",
    "\n",
    "    # Iterate through all articles in the forward_index\n",
    "    for doc_id, data in forward_index.items():\n",
    "        # Look at all words in an article\n",
    "        for word_id in data:\n",
    "            # Calculate the barrel number for that word\n",
    "            barrel_no = int(word_id) // 10000\n",
    "            barrel_filename = f\"barrel_{barrel_no}.json\"\n",
    "            \n",
    "            # Check if that barrel exists, if not then create it\n",
    "            barrel_path = os.path.join(r\"Files\\Barrels\", barrel_filename)\n",
    "            if not os.path.exists(barrel_path):\n",
    "                with open(barrel_path, \"w\") as file:\n",
    "                    json.dump(dict(), file)\n",
    "                # Load the newly created barrel\n",
    "                barrels.append(load_data_from_json(barrel_path))\n",
    "                barrel_files.append(barrel_filename)\n",
    "            # update the word_id\n",
    "            word_id_new = int(word_id) % 10000\n",
    "            # If that word is not already in that barrel\n",
    "            if word_id_new not in barrels[barrel_no]:\n",
    "                # Then create a dict at that word_id\n",
    "                barrels[barrel_no][word_id_new] = dict()\n",
    "            # And add the doc_id for that word along with frequency if it is not already there\n",
    "            if doc_id not in barrels[barrel_no][word_id_new]:\n",
    "                barrels[barrel_no][word_id_new].update({doc_id: data[word_id]})\n",
    "\n",
    "    # Sort the barrels\n",
    "    for i, barrel in enumerate(barrels):\n",
    "        sorted_data = {}\n",
    "        for outer_key, inner_dict in barrel.items():\n",
    "            sorted_inner = dict(sorted(inner_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "            sorted_data[outer_key] = sorted_inner\n",
    "            barrels[i] = sorted_data\n",
    "    # Update all barrels\n",
    "    i = 0\n",
    "    for barrel in barrel_files:\n",
    "        with open(os.path.join(r\"Files\\Barrels\", barrel), \"w\") as file:\n",
    "            json.dump(barrels[i], file)\n",
    "            i += 1\n",
    "    \n",
    "    # Clear the forward_index\n",
    "    with open(r\"Files\\forward_index.json\", \"w\") as file:\n",
    "        json.dump(dict(), file)\n",
    "\n",
    "\n",
    "def rank_results(search_result): \n",
    "     # Rank these documents\n",
    "    # Sort the dictionary by values (descending order)\n",
    "    sorted_tuples = sorted(search_result.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convert the sorted list of tuples back to a dictionary\n",
    "    ranked_result = dict(sorted_tuples)\n",
    "    # Extract the article ids\n",
    "    ranked_articles = ranked_result.keys()\n",
    "    ranked_articles = list(ranked_articles)\n",
    "    \n",
    "    return ranked_articles\n",
    "\n",
    "def add_content(data, new_article):\n",
    "    article_id = str(len(data[\"index\"]))\n",
    "    data[\"index\"][article_id] = len(data[\"index\"])\n",
    "    data[\"source\"][article_id] = new_article[0]\n",
    "    data[\"title\"][article_id] = new_article[1]\n",
    "    data[\"content\"][article_id] = new_article[2]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cba293-fb92-4c24-b2a6-66b1d21aa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the data\n",
    "#json_file_path = r\"Files\\articles_sampled_10000.json\"\n",
    "#json_data = load_data_from_json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4e84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Files directory if it does not already exist\n",
    "#if \"Files\" not in os.listdir():\n",
    "#    os.mkdir(\"Files\")\n",
    "#    os.mkdir(r\"Files\\Barrels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633c694b-6bbe-4652-a240-d93673148bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#build_forward_index(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63487818-d824-4ffd-8633-a82588cf6acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#build_inverted_index_with_barrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2cd59d-d135-4cb2-92bf-d94d488ba6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "barrels = []\n",
    "barrel_files = os.listdir(r\"Files\\Barrels\")\n",
    "# Load all barrels that currently exist\n",
    "for barrel in barrel_files:\n",
    "    barrels.append(load_data_from_json(os.path.join(r\"Files\\Barrels\", barrel)))\n",
    "    \n",
    "# Load lexicon\n",
    "lexicon = load_data_from_json(r\"Files\\lexicon.json\")\n",
    "# Load the documents\n",
    "documents = load_data_from_json(r\"Files\\documents.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ed9c87b-9610-4963-8b46-47d35b661043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [16/Dec/2023 19:58:05] \"POST /add HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/search_1\", methods=[\"GET\"], endpoint='single_word_search')\n",
    "def single_word_search():\n",
    "    word = request.args.get('word')\n",
    "\n",
    "    # Lemmatize the word\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    # Find the id of the word in lexicon\n",
    "    try:\n",
    "        word_id = lexicon.index(word)\n",
    "        # Calculate the barrel of the word\n",
    "        barrel_no = word_id // 10000\n",
    "        # Update the word_id\n",
    "        word_id = word_id % 10000\n",
    "        # Find out in which documents does the word appear\n",
    "        search_result = barrels[barrel_no][str(word_id)]\n",
    "    except:\n",
    "        search_result = None\n",
    "    \n",
    "    if search_result is None: \n",
    "        return jsonify(article_ids=[], titles=[], urls=[])\n",
    "\n",
    "    article_ids = list(search_result.keys())\n",
    "    titles = [documents[article]['title'] for article in article_ids]\n",
    "    urls = [documents[article]['url'] for article in article_ids]\n",
    "    \n",
    "    json_response = jsonify(article_ids=article_ids, titles=titles, urls=urls)\n",
    "\n",
    "    return json_response\n",
    "\n",
    "@app.route(\"/search_2\", methods=[\"GET\"], endpoint='multi_word_search')\n",
    "def multi_word_search(): \n",
    "    query = request.args.get('word')\n",
    "    result = []\n",
    "\n",
    "    # Preprocess the query\n",
    "    words = pre_process_string(query)\n",
    "\n",
    "    # Remove those words that are not in lexicon\n",
    "    words = [word for word in words if word in lexicon]\n",
    "    # Convert each word to its word_id\n",
    "    word_ids = [lexicon.index(word) for word in words]\n",
    "    # Calculate barrel_no of each word and its index in that barrel\n",
    "    barrel_nos = [word_id // 10000 for word_id in word_ids]\n",
    "    word_ids = [word_id % 10000 for word_id in word_ids]\n",
    "\n",
    "    # Check the first word\n",
    "    if word_ids:\n",
    "        result = barrels[barrel_nos[0]][str(word_ids[0])]\n",
    "        # Check the rest of the words\n",
    "        for i, word_id in enumerate(word_ids[1:], start = 1):\n",
    "            # Produce the result for current word\n",
    "            current_result = barrels[barrel_nos[i]][str(word_id)]\n",
    "            # Include those articles that are also in the result of current word\n",
    "            result.update({d:result[d]+current_result[d] for d in result.keys() if d in current_result.keys()})\n",
    "\n",
    "    if result is None:\n",
    "        return jsonify(article_ids=[], titles=[], urls=[])\n",
    "    \n",
    "    # rank the results\n",
    "    result = rank_results(result)\n",
    "\n",
    "    article_ids = result\n",
    "    titles = [documents[article]['title'] for article in article_ids]\n",
    "    urls = [documents[article]['url'] for article in article_ids]\n",
    "\n",
    "    json_response = jsonify(article_ids=article_ids, titles=titles, urls=urls)\n",
    "\n",
    "    return json_response\n",
    "\n",
    "\n",
    "@app.route(\"/gen\", methods=[\"GET\"], endpoint='genai_tool')\n",
    "def genai_tool():\n",
    "    word = request.args.get('word') \n",
    "    image = pipe(prompt=word, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "\n",
    "    image2 = image.convert(\"RGB\")\n",
    " \n",
    "    image_bytes_io = BytesIO()\n",
    "    image2.save(image_bytes_io, format=\"PNG\")\n",
    "    image_bytes = image_bytes_io.getvalue()\n",
    "    \n",
    "    image_base64 = base64.b64encode(image_bytes).decode('utf-8')  \n",
    "\n",
    "    json_response = {\n",
    "        'word': word,\n",
    "        'image': image_base64\n",
    "    }\n",
    "\n",
    "    return jsonify(json_response)\n",
    "\n",
    "UPLOAD_FOLDER = 'Files\\\\Uploads'  \n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "@app.route(\"/add\", methods=[\"POST\"], endpoint='add_content')\n",
    "def add_content():\n",
    "    title = request.form.get('title')\n",
    "    url = request.form.get('url')\n",
    "    content = request.form.get('content')\n",
    "    file = request.files.get('file') \n",
    " \n",
    "    filename = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)\n",
    "    file.save(filename)\n",
    "\n",
    "    global barrels, lexicon, documents\n",
    "    # Check if a file is uploaded\n",
    "    if file:\n",
    "        try:\n",
    "            # Load the file\n",
    "            data = load_data_from_json(filename) \n",
    "        except:\n",
    "            return jsonify({\"message\": \"Error loading file\"}), 500\n",
    "    else:\n",
    "        # Check if the url, title and content are correct\n",
    "        if url and title and content:\n",
    "            # Validate the url\n",
    "            if validators.url(url) != True:\n",
    "                return jsonify({\"message\": \"Please provide a valid url\"}), 400\n",
    "            # Load the data\n",
    "            data = [{\"title\":title, \"content\":content, \"url\":url}]\n",
    "        else:\n",
    "            return jsonify({\"message\": \"Please provide a file or url, title and content\"}), 400\n",
    "\n",
    "\n",
    "    return jsonify({\"message\": \"Successfully added file or hard code data but did not make the indexes yet\"}), 200\n",
    "    # build forward and inverted index on it\n",
    "    try:\n",
    "        build_forward_index(data)\n",
    "    except:\n",
    "        return jsonify({\"message\": \"Error building forward index\"}), 500\n",
    "    build_inverted_index_with_barrels()\n",
    "\n",
    "    # Reload barrels, documents, and lexicon\n",
    "    barrel_files = os.listdir(r\"Files\\\\Barrels\")\n",
    "    # Load all barrels that currently exist\n",
    "    for i, barrel in enumerate(barrel_files):\n",
    "        try:\n",
    "            barrels[i] = (load_data_from_json(os.path.join(r\"Files\\\\Barrels\", barrel)))\n",
    "        except:\n",
    "            barrels.append(load_data_from_json(os.path.join(r\"Files\\\\Barrels\", barrel)))\n",
    "        \n",
    "    # Load lexicon\n",
    "    lexicon = load_data_from_json(r\"Files\\\\lexicon.json\")\n",
    "    # Load the documents\n",
    "    documents = load_data_from_json(r\"Files\\\\documents.json\")\n",
    "    \n",
    "    return jsonify({\"message\": \"Successfully added content\"}), 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74549c9c-ff22-451d-8985-123af4b4f249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
