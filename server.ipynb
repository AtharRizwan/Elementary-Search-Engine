{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import csv\n",
    "import json\n",
    "import flask_cors, flask\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from flask_cors import CORS\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "import base64\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  9.66it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.24.0\",\n",
       "  \"_name_or_path\": \"stabilityai/sd-turbo\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, key, value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "\n",
    "    def insert(self, key, value):\n",
    "        new_node = Node(key, value)\n",
    "        new_node.next = self.head\n",
    "        self.head = new_node\n",
    "\n",
    "    def search(self, key):\n",
    "        current = self.head\n",
    "        while current:\n",
    "            if current.key == key:\n",
    "                return current.value\n",
    "            current = current.next\n",
    "        return None\n",
    "    \n",
    "    def display(self):\n",
    "        values = []\n",
    "        current = self.head\n",
    "        while current:\n",
    "            values.append((current.key, current.value))\n",
    "            current = current.next\n",
    "        return values\n",
    "\n",
    "class HashTable:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.table = [None] * size\n",
    "\n",
    "    def hash_function(self, key):\n",
    "        return hash(key) % self.size\n",
    "\n",
    "    def insert(self, key, value): \n",
    "        index = self.hash_function(key)\n",
    "        if self.table[index] is None:\n",
    "            self.table[index] = LinkedList()\n",
    "        self.table[index].insert(key, value)\n",
    "\n",
    "    def search(self, key):\n",
    "        index = self.hash_function(key)\n",
    "        if self.table[index] is not None:\n",
    "            return self.table[index].search(key)\n",
    "        return None\n",
    "    \n",
    "    def display(self):\n",
    "        table_values = []\n",
    "        for i, linked_list in enumerate(self.table):\n",
    "            if linked_list is not None:\n",
    "                values = linked_list.display()\n",
    "                for key, value in values:\n",
    "                    table_values.append((i, key, value))\n",
    "        return table_values\n",
    "    \n",
    "def build_forward_index(data):\n",
    "    forward_index = HashTable(size=100)\n",
    "    for article_id, title_words in data[\"title\"].items(): \n",
    "        if article_id in data[\"content\"]:\n",
    "            content_words = data[\"content\"][article_id] \n",
    "            words = title_words + content_words\n",
    "\n",
    "            forward_index.insert(article_id, words)\n",
    "\n",
    "    return forward_index \n",
    " \n",
    "\n",
    "def build_inverted_index(data):\n",
    "    inverted_index = HashTable(size=100)\n",
    "    for article_id, words in data[\"content\"].items(): \n",
    "        topic = data[\"title\"][article_id]\n",
    "        url = data[\"url\"][article_id]\n",
    "    \n",
    "        for word in words:\n",
    "            if inverted_index.search(word):\n",
    "                inverted_index.search(word).append({\"article_id\": article_id, \"title\": topic, \"url\": url})\n",
    "            else:\n",
    "                inverted_index.insert(word, [{\"article_id\": article_id, \"title\": topic, \"url\": url}])\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "def single_word_search(inverted_index, word):\n",
    "    return inverted_index.search(word)\n",
    "\n",
    "def multi_word_search_2(inverted_index, query):\n",
    "    result = set()\n",
    "    words = query.split()\n",
    "    if words:\n",
    "        result = set(inverted_index.search(words[0]))\n",
    "        for word in words[1:]:\n",
    "            result.intersection_update(inverted_index.search(word))\n",
    "    return list(result)\n",
    "\n",
    "def multi_word_search(inverted_index, query):\n",
    "    result = []\n",
    "    words = query.split()\n",
    "\n",
    "    if words:\n",
    "        result = inverted_index.search(words[0]) if inverted_index.search(words[0]) else []\n",
    "        for word in words[1:]:\n",
    "            current_result = inverted_index.search(word)\n",
    "            if current_result:\n",
    "                result = [d for d in result if d in current_result]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def rank_results(input_list): \n",
    "    counts = Counter(tuple(item['article_id']) for item in input_list) \n",
    "    sorted_list = sorted(input_list, key=lambda item: counts[tuple(item['article_id'])], reverse=True) \n",
    "    unique_set = set()\n",
    "    output = [item for item in sorted_list if tuple(item['title']) not in unique_set and not unique_set.add(tuple(item['article_id']))]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def display_results(results):\n",
    "    for article_id in results:\n",
    "        print(f\"Article {article_id}: {data['content'][article_id]}\")\n",
    "\n",
    "def add_content(data, new_article):\n",
    "    article_id = str(len(data[\"index\"]))\n",
    "    data[\"index\"][article_id] = len(data[\"index\"])\n",
    "    data[\"source\"][article_id] = new_article[0]\n",
    "    data[\"title\"][article_id] = new_article[1]\n",
    "    data[\"content\"][article_id] = new_article[2]\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_duplicates(input_list, key = None):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    \n",
    "    for item in input_list: \n",
    "        hashable_item = frozenset(item.items()) if key is None else item.get(key)\n",
    "        \n",
    "        if hashable_item not in seen:\n",
    "            seen.add(hashable_item)\n",
    "            result.append(item)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "json_file_path = \"Files\\cleaned.json\"\n",
    "json_data = load_data_from_json(json_file_path)\n",
    "\n",
    "forward_index = build_forward_index(json_data)\n",
    "inverted_index = build_inverted_index(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
      "127.0.0.1 - - [09/Dec/2023 00:30:10] \"GET /gen?word=business HTTP/1.1\" 200 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:30:39] \"GET /gen?word=Study HTTP/1.1\" 200 -\n",
      "[2023-12-09 00:30:54,350] ERROR in app: Exception on /search_1 [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\flask\\app.py\", line 1455, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\flask\\app.py\", line 869, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\flask_cors\\extension.py\", line 176, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "                                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\flask\\app.py\", line 867, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\flask\\app.py\", line 852, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haris\\AppData\\Local\\Temp\\ipykernel_20492\\3940127111.py\", line 4, in single_word_search\n",
      "    ranked_results = rank_results(remove_duplicates(inverted_index.search(word), key=\"article_id\"))\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\haris\\AppData\\Local\\Temp\\ipykernel_20492\\963884784.py\", line 143, in remove_duplicates\n",
      "    for item in input_list:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "127.0.0.1 - - [09/Dec/2023 00:30:54] \"GET /search_1?word=Ronaldo HTTP/1.1\" 500 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.43it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:32:33] \"GET /gen?word=hi HTTP/1.1\" 200 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:33:19] \"GET /gen?word=building HTTP/1.1\" 200 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:34:25] \"GET /gen?word=hello HTTP/1.1\" 200 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.07it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:35:00] \"GET /gen?word=hiohnla HTTP/1.1\" 200 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:35:23] \"GET /gen?word=beach HTTP/1.1\" 200 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:36:22] \"GET /gen?word=high%20rise%20buildings HTTP/1.1\" 200 -\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "127.0.0.1 - - [09/Dec/2023 00:37:50] \"GET /gen?word=a%20cat%20jumping%20in%20space HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2023 00:38:11] \"GET /search_1?word=for HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Dec/2023 00:38:20] \"GET /search_1?word=syndrome HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "@app.route(\"/search_1\", methods=[\"GET\"], endpoint='single_word_search')\n",
    "def single_word_search():\n",
    "    word = request.args.get('word')\n",
    "    ranked_results = rank_results(remove_duplicates(inverted_index.search(word), key=\"article_id\"))\n",
    "\n",
    "    article_ids = [result['article_id'] for result in ranked_results]\n",
    "    titles = [result['title'] for result in ranked_results]\n",
    "    urls = [result['url'] for result in ranked_results] \n",
    "    json_response = jsonify(article_ids=article_ids, titles=titles, urls=urls)\n",
    "\n",
    "    return json_response\n",
    "\n",
    "@app.route(\"/search_2\", methods=[\"GET\"], endpoint='multi_word_search')\n",
    "def multi_word_search(): \n",
    "    query = request.args.get('word')\n",
    "    result = []\n",
    "    words = query.split()\n",
    "\n",
    "    if words:\n",
    "        result = inverted_index.search(words[0]) if inverted_index.search(words[0]) else []\n",
    "        for word in words[1:]:\n",
    "            current_result = inverted_index.search(word)\n",
    "            if current_result:\n",
    "                result = [d for d in result if d in current_result]\n",
    "\n",
    "    ranked_results = rank_results(remove_duplicates(result, key=\"article_id\"))\n",
    "\n",
    "    article_ids = [result['article_id'] for result in ranked_results]\n",
    "    titles = [result['title'] for result in ranked_results]\n",
    "    urls = [result['url'] for result in ranked_results]\n",
    "\n",
    "    json_response = jsonify(article_ids=article_ids, titles=titles, urls=urls)\n",
    "\n",
    "    return json_response \n",
    "\n",
    "\n",
    "@app.route(\"/gen\", methods=[\"GET\"], endpoint='genai_tool')\n",
    "def genai_tool():\n",
    "    word = request.args.get('word') \n",
    "    image = pipe(prompt=word, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "\n",
    "    # Save the image to the \"Photos\" folder\n",
    "    folder_path = os.path.join(os.getcwd(), \"frontend-gui/public/Photos\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    " \n",
    "    file_name = \"image\"\n",
    "\n",
    "    file_path = os.path.join(folder_path, f\"{file_name}.jpeg\")  # You can change the extension to 'png' if needed\n",
    "    image.save(file_path)\n",
    "\n",
    "    # Convert the image to base64\n",
    "    image_base64 = base64.b64encode(image.tobytes()).decode('utf-8')\n",
    "\n",
    "\n",
    "    # Create a JSON response with the base64-encoded image\n",
    "    json_response = {\n",
    "        'word': word,\n",
    "        'image': image_base64\n",
    "    }\n",
    "\n",
    "    return jsonify(json_response)\n",
    "\n",
    "\n",
    "@app.route(\"/add\", methods=[\"GET\"])\n",
    "def add_content(data, new_article):\n",
    "    article_id = str(len(data[\"index\"]))\n",
    "    data[\"index\"][article_id] = len(data[\"index\"])\n",
    "    data[\"source\"][article_id] = new_article[0]\n",
    "    data[\"title\"][article_id] = new_article[1]\n",
    "    data[\"content\"][article_id] = new_article[2]\n",
    "\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
